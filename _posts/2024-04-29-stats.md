---
title: Notes on Statistics
date: 2024-04-29 10:53:00 +0330
categories: [Notes, Stats]
tags: [stats, estimation]
math: true
---

[Statistics for Applications](https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/)

## Statistical Modeling

$$
\text{Complicated Process} = \text{Simple Process} + \text{Random Noise}
$$

Good modeling is choosing a plausible simple process and noise distribution.

## LLN & CLT

Let $$X, X_1, ... X_n$$ be i.i.d. random variables with $$\mu = \mathbb{E}[X]$$ and $$\sigma^2 = \mathbb{V}[X]$$.

### Law of Large Number

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i~\underset{n\rightarrow\infty}{\overset{a.s.,\mathbb{P}}{\longrightarrow}}~\mu
$$

### Central Limit Theorem

$$
\sqrt{n}\left(\frac{\bar{X}_n - \mu}{\sigma}\right)~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(0, 1)
$$

or:

$$
\bar{X}_n~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(\mu, \frac{\sigma^2}{n})
$$

or:

$$
\lim_{n\rightarrow\infty} \mathbb{P}[\bar{X}_n-\mu \le x] = \Phi(\frac{\sqrt n x}{\sigma})
$$

#### Confidence Interval

So for large enough $$n$$ we can derive:

$$
\mathbb{P}[|\bar{X}_n-\mu| \le \epsilon] \approx 2\Phi(\frac{\sqrt n \epsilon}{\sigma}) - 1 = 1 - \alpha
$$

Solving for $$\epsilon$$ we have:

$$
\epsilon = \frac{\sigma}{\sqrt{n}}q\left(1 - \frac{\alpha}{2}\right)
$$

where $$q = \Phi^{-1}$$ is the quantile function of the standard gaussian distribution.

In another form:

$$
\mathbb{P}\left[\mu - \epsilon < \bar{X}_n \le \mu+\epsilon\right] = 1 - \alpha
$$

To replace unknown parameter $$\sigma$$ in the bound we can use its estimate $$\hat{\sigma}$$ by Slutsky's Theorem.

### The Delta Method

If CLT holds for $$\bar{X}_n$$ and $$g: \mathbb{R}\rightarrow\mathbb{R}$$ is differentiable at $$\mu$$:

$$
\sqrt n \left(g(\bar{X}_n) - g(\mu)\right)\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}\mathcal N(0, g'(\mu)^2\sigma^2)
$$

### Hoeffding's Inequality

...

## Parametric Inference

### Statistical Model

Given observed outcome of a statistical experiment be a sample $$X_1, ..., X_n$$ of n i.i.d. random variables in some measurable space $$\mathcal X$$ and denote $$\mathbb P$$ as their common distribution. A __Statistical Model__ associated to that statistical experiment is a pair:

$$
\left(\mathcal X, \{\mathbb{P}_\theta\}_{\theta\in\Theta}\right)
$$

- Usually it is assumed that the statistical model is well specified: $$\mathbb{P}=\mathbb{P}_{\theta^*}$$ for some $$\theta^*\in\Theta$$.
- The aim of statistical experiment is to estimate $$\theta^*$$ or check its properties if they have special meaning.

### Parameter Estimation

Given an observed sample $$X_1, ..., X_n$$ and a statistical model $$\left(\mathcal X, \{\mathbb{P}_\theta\}_{\theta\in\Theta}\right)$$, estimate the parameter $$\theta^*$$

__Statistic__: Any measurable function of the samples.

__Estimator of__ $$\theta^*$$: An statistic that doesn't depend on $$\theta$$.

An estimator $$\hat{\theta}_n$$ is __consistent__ (strong and weak) iff:

$$
\hat{\theta}_n~\underset{n \rightarrow \infty}{\overset{a.s., \mathbb{P}}{\longrightarrow}}~\theta^*
$$

w.r.t. $$\mathbb{P}_{\theta^*}$$

### Bias of an Estimator

$$
bias = \mathbb{E}[\hat{\theta}_n] - \theta^*
$$

### Variance of an Estimator

$$
variance = \mathbb{E}\left[\left(\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]\right)^2\right]
$$

### (Quadratic) Risk of an Estimator

$$
\mathbb{E}[|\hat{\theta}_n - \theta^*|^2] = bias^2 + variance
$$

### Confidence Intervals

Let $$(\mathcal{X}, \{\mathbb{P}_{\theta}\}_{\theta\in\Theta})$$ be a statistical model, and let $$\alpha\in(0, 1)$$:

__Confidence Interval (C.I.)__ of level $$1 - \alpha$$ is any random interval $$\mathcal{I}$$ whose boundaries do not depend on $$\theta$$ such that:

$$
\mathbb{P}_\theta[\theta\in\mathcal{I}] \ge 1 - \alpha, \qquad \forall \theta\in\Theta
$$

__Confidence Interval (C.I.)__ of __asymptotic__ level of $$1 - \alpha$$ is any random interval $$\mathcal{I}$$ whose boundaries do not depend on $$\theta$$ such that:

$$
\lim_{n\rightarrow\infty}\mathbb{P}_\theta[\theta\in\mathcal{I}] \ge 1 - \alpha, \qquad \forall \theta\in\Theta
$$

> Note that $$\mathcal{I}$$  should not depend on parameter $$\theta$$. It can be either replaced by its estimate (by Slutsky), or a tight upper bound of it.
{: .prompt-tip }

### Maximum Likelihood

...

## Parametric Hypothesis Testing

Consider $$X_1, ... X_n$$ n i.i.d. samples drawn from random variable $$X\sim\mathbb P_X$$ with support $$\mathcal X$$, and a parametric model $$\{\mathbb{P}_\theta\}_{\theta\in\Theta}$$.

Let $$\Theta_0$$ and $$\Theta_1$$ be __disjoint__ subsets of $$\Theta$$. Consider two hypotheses:

1. __Null hypothesis:__ $$H_0: \theta^*\in\Theta_0$$
1. __Alternate Hypothesis:__ $$H_1: \theta^*\in\Theta_1$$

We want to decide whether to reject null hypothesis (look for evidence against $$H_0$$ in data). The data is only used to try to disprove $$H_0$$. Null hypothesis is the "status quo". Lack of evidence against $$H_0$$ does not mean that $$H_0$$ is true. Alternate hypothesis is a "discovery" that goes against the status quo null hypothesis. We want to test the null hypothesis against the alternate hypothesis.

> Innocent untill proven guilty.
{: .prompt-tip }

### Test

A test is a _statistic_ in the form of an identity function:

$$
\psi(X_1, ..., X_n) = \mathbb{I}\{T_n > c\}
$$

A test is defined on a __test statistic__ $$T_n$$.

- If $$\psi = 0$$, $$H_0$$ is not rejected.
- If $$\psi = 1$$, $$H_0$$ is rejected.

### Rejection Region

Set of samples that reject the null hypothesis:

$$
R_\psi = \{X_1, ...,X_n\in\mathcal X^n: \psi(X_1, ...,X_n)=1\}
$$

equivalently:

$$
R_\psi = \{X_1, ...,X_n\in\mathcal X^n: T_n > c\}
$$

### Error Types

We only control one of the errors.

#### Type 1 Error

Null hypothesis is true, but we rejected it in favor of alternate. Probability that the test $$\psi = 1$$ under $$\mathbb{P}_\theta$$, where $$\theta \in \Theta_0$$:

$$
\alpha_\psi(\theta) = \mathbb{P}_\theta[\psi=1],\qquad\forall \theta\in\Theta_0
$$

#### Type 2 Error

Alternate hypothesis is true, but we failed to reject null hypothesis:

$$
\beta_\psi(\theta) = \mathbb{P}_\theta[\psi=0],\qquad\forall \theta\in\Theta_1
$$

### Power of the Test

Probability of correct rejection of null hypothesis in worst case scenario:

$$
\pi_\psi = \inf_{\theta \in \Theta_1} (1 - \beta_\psi(\theta))
$$
