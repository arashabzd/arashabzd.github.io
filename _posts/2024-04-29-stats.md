---
title: Notes on Statistics
date: 2024-04-29 10:53:00 +0330
categories: [Notes, Stats]
tags: [stats, estimation]
math: true
---

## Statistical Modeling

$$
\text{Complicated Process} = \text{Simple Process} + \text{Random Noise}
$$

Good modeling is choosing a plausible simple process and noise distribution.

## LLN & CLT

Let $$X, X_1, ... X_n$$ be i.i.d. random variables with $$\mu = \mathbb{E}[X]$$ and $$\sigma^2 = \mathbb{V}[X]$$.

### Law of Large Number

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i~\underset{n\rightarrow\infty}{\overset{a.s.,\mathbb{P}}{\longrightarrow}}~\mu
$$

### Central Limit Theorem

$$
\sqrt{n}\left(\frac{\bar{X}_n - \mu}{\sigma}\right)~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(0, 1)
$$

or:

$$
\bar{X}_n~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(\mu, \frac{\sigma^2}{n})
$$

or:

$$
\lim_{n\rightarrow\infty} \mathbb{P}[\bar{X}_n-\mu \le x] = \Phi(\frac{\sqrt n x}{\sigma})
$$

### Confidence Interval

So for large enough $$n$$ we can derive:

$$
\mathbb{P}[|\bar{X}_n-\mu| \le \epsilon] \approx 2\Phi(\frac{\sqrt n \epsilon}{\sigma}) - 1 = 1 - \alpha
$$

Solving for $$\epsilon$$ we have:

$$
\epsilon = \frac{\sigma}{\sqrt{n}}q\left(1 - \frac{\alpha}{2}\right)
$$

where $$q = \Phi^{-1}$$ is the quantile function of the standard gaussian distribution.

In another form:

$$
\mathbb{P}\left[\mu - \epsilon < \bar{X}_n \le \mu+\epsilon\right] = 1 - \alpha
$$

To replace unknown parameter $$\sigma$$ in the bound we can use its estimate $$\hat{\sigma}$$ by Slutsky's Theorem.

### The Delta Method

If CLT holds for $$\bar{X}_n$$ and $$g: \mathbb{R}\rightarrow\mathbb{R}$$ is differentiable at $$\mu$$:

$$
\sqrt n \left(g(\bar{X}_n) - g(\mu)\right)\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}\mathcal N(0, g'(\mu)^2\sigma^2)
$$

### Hoeffding's Inequality

...

## Parametric Inference

### Parameter Estimation

Given observed samples $$X_1, ..., X_n$$ and a parametric model $$\{\mathbb{P}_\theta\}_{\theta\in\Theta}$$, one wants to estimate the parameter $$\theta$$.

__Statistic__: Any measurable function of the samples.

__Estimator of__ $$\theta$$: An statistic that doesn't depend on $$\theta$$.

An estimator $$\hat{\theta}_n$$ is __consistent__ (strong and weak) iff:

$$
\hat{\theta}_n~\underset{n \rightarrow \infty}{\overset{a.s., \mathbb{P}}{\longrightarrow}}~\theta^*
$$

w.r.t. $$\mathbb{P}_{\theta^*}$$

### Bias of an Estimator

$$
bias = \mathbb{E}[\hat{\theta}_n] - \theta^*
$$

### Variance of an Estimator

$$
variance = \mathbb{E}\left[\left(\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]\right)^2\right]
$$

### (Quadratic) Risk of an Estimator

$$
\mathbb{E}[|\hat{\theta}_n - \theta^*|^2] = bias^2 + variance
$$

### Maximum Likelihood

...

## Parametric Hypothesis Testing

Consider a sample of $$X, X_1, ... X_n$$ i.i.d. random variables and a parametric model $$\{\mathbb{P}_\theta\}_{\theta\in\Theta}$$.

Let $$\Theta_0$$ and $$\Theta_1$$ be __disjoint__ subsets of $$\Theta$$. Consider two hypotheses:

1. __Null hypothesis:__ $$H_0: \theta^*\in\Theta_0$$
1. __Alternate Hypothesis:__ $$H_1: \theta^*\in\Theta_1$$

We want to decide whether to reject null hypothesis (look for evidence against $$H_0$$ in data). The data is only used to try to disprove $$H_0$$. Null hypothesis is the "status quo". Lack of evidence against $$H_0$$ does not mean that $$H_0$$ is true. Alternate hypothesis is a "discovery" that goes against the status quo null hypothesis. We want to test the null hypothesis against the alternate hypothesis.

> Innocent untill proven guilty.

### Test

A test is a _statistic_ in the form of an identity function $$\psi\in\{0, 1\}$$ such that:

- If $$\psi = 0$$, $$H_0$$ is not rejected.
- If $$\psi = 1$$, $$H_0$$ is rejected.

### Rejection Region

Set of $$n$$ samples from sample space $$\mathcal X^n$$ that rejects the null hypothesis.

$$
R_\psi = \{x\in\mathcal X^n: \psi(x)=1\}
$$

### Error Types

We only control one of the errors.

#### Type 1 Error

Null hypothesis is true, but we rejected it in favor of alternate. Probability that the test $$\psi = 1$$ under $$\mathbb{P}_\theta$$, where $$\theta \in \Theta_0$$:

$$
\alpha_\psi(\theta) = \mathbb{P}_\theta[\psi=1],\qquad\forall \theta\in\Theta_0
$$

#### Type 2 Error

Alternate hypothesis is true, but we failed to reject null hypothesis:

$$
\beta_\psi(\theta) = \mathbb{P}_\theta[\psi=0],\qquad\forall \theta\in\Theta_1
$$

### Power of the Test

Probability of correct rejection of null hypothesis in worst case scenario:

$$
\pi_\psi = \inf_{\theta \in \Theta_1} (1 - \beta_\psi(\theta))
$$
