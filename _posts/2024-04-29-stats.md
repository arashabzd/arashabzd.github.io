---
title: Notes on Statistics
date: 2024-04-29 10:53:00 +0330
categories: [Notes, Stats]
tags: [stats, estimation]
math: true
---

## Statistical Modeling

$$
\text{Complicated Process} = \text{Simple Process} + \text{Random Noise}
$$

Good modeling is choosing a plausible simple process and noise distribution.

## LLN & CLT

Let $$X, X_1, ... X_n$$ be i.i.d. random variables with $$\mu = \mathbb{E}[X]$$ and $$\sigma^2 = \mathbb{V}[X]$$.

### Law of Large Number

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i~\underset{n\rightarrow\infty}{\overset{a.s.,\mathbb{P}}{\longrightarrow}}~\mu
$$

### Central Limit Theorem

$$
\sqrt{n}\left(\frac{\bar{X}_n - \mu}{\sigma}\right)~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(0, 1)
$$

or:

$$
\bar{X}_n~\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}~\mathcal{N}(\mu, \frac{\sigma^2}{n})
$$

or:

$$
\lim_{n\rightarrow\infty} \mathbb{P}[\bar{X}_n-\mu \le x] = \Phi(\frac{\sqrt n x}{\sigma})
$$

### Confidence Interval

So for large enough $$n$$ we can derive:

$$
\mathbb{P}[|\bar{X}_n-\mu| \le \epsilon] \approx 2\Phi(\frac{\sqrt n \epsilon}{\sigma}) - 1 = 1 - \alpha
$$

Solving for $$\epsilon$$ we have:

$$
\epsilon = \frac{\sigma}{\sqrt{n}}q\left(1 - \frac{\alpha}{2}\right)
$$

where $$q = \Phi^{-1}$$ is the quantile function of the standard gaussian distribution.

In another form:

$$
\mathbb{P}\left[\mu - \epsilon < \bar{X}_n \le \mu+\epsilon\right] = 1 - \alpha
$$

To replace unknown parameter $$\sigma$$ in the bound we can use its estimate $$\hat{\sigma}$$ by Slutsky's Theorem.

### The Delta Method

If CLT holds for $$\bar{X}_n$$ and $$g: \mathbb{R}\rightarrow\mathbb{R}$$ is differentiable at $$\mu$$:

$$
\sqrt n \left(g(\bar{X}_n) - g(\mu)\right)\underset{n \rightarrow \infty}{\overset{(d)}{\longrightarrow}}\mathcal N(0, g'(\mu)^2\sigma^2)
$$

## Parametric Inference

### Parameter Estimation

Given observed samples $$X_1, ..., X_n$$ and a parametric model $$\{\mathbb{P}_\theta\}_{\theta\in\Theta}$$, one wants to estimate the parameter $$\theta$$.

__Statistic__: Any measurable function of the samples.

__Estimator of__ $$\theta$$: An statistic that doesn't depend on $$\theta$$.

An estimator $$\hat{\theta}_n$$ is __consistent__ (strong and weak) iff:

$$
\hat{\theta}_n~\underset{n \rightarrow \infty}{\overset{a.s., \mathbb{P}}{\longrightarrow}}~\theta^*
$$

w.r.t. $$\mathbb{P}_{\theta^*}$$

### Bias of an Estimator

$$
bias = \mathbb{E}[\hat{\theta}_n] - \theta^*
$$

### Variance of an Estimator

$$
variance = \mathbb{E}\left[\left(\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]\right)^2\right]
$$

### (Quadratic) Risk of an Estimator

$$
\mathbb{E}[|\hat{\theta}_n - \theta^*|^2] = bias^2 + variance
$$
